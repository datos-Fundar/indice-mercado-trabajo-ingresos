{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "pathdata = '/home/daniu/Documentos/fundar/indice-mercado-trabajo-ingresos/'\n",
    "pathdata = '/Users/danielarisaro/Documents/fundar/indice-mercado-trabajo-ingresos/'\n",
    "pathdata = '/home/daniufundar/Documents/Fundar/indice-mercado-trabajo-ingresos/'\n",
    "\n",
    "df_people_2021 = pd.read_csv(pathdata + 'data_input/personas_tot_urb_3T_21.txt', delimiter=';', low_memory=False)\n",
    "df_people_2022 = pd.read_csv(pathdata + 'data_input/personas_tot_urb_3T_22.txt', delimiter=';', low_memory=False)\n",
    "\n",
    "df_houses_2021 = pd.read_csv(pathdata + 'data_input/hogar_tot_urb_3T_2021.txt', delimiter=';', low_memory=False)\n",
    "df_houses_2022 = pd.read_csv(pathdata + 'data_input/hogar_tot_urb_3T_2022.txt', delimiter=';', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_calculate_people(df_year1, df_year2, year1, year2, conversion_factor):\n",
    "\n",
    "    \"\"\"\n",
    "    Combina dos DataFrames de dos anios de la base de personas de la EPH utilizando las columnas 'CODUSU' y 'COMPONENTE'. Realiza cálculos en los datos fusionados.\n",
    "\n",
    "    Args:\n",
    "        df_year1 (pandas.DataFrame): DataFrame para el año 1.\n",
    "        df_year2 (pandas.DataFrame): DataFrame para el año 2.\n",
    "        year1 (int): Año de los datos de df_year1.\n",
    "        year2 (int): Año de los datos de df_year2.\n",
    "        conversion_factor (float): Factor de conversión para la transformación de datos.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame fusionado con los valores calculados. Base pool de base personas sin replicas.\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    merged_df = pd.merge(df_year1[['CODUSU', 'COMPONENTE']], df_year2[['CODUSU', 'COMPONENTE']],\n",
    "                         on=['CODUSU', 'COMPONENTE'], how='outer', indicator=True)\n",
    "\n",
    "    left_only_rows = merged_df[merged_df['_merge'] == 'left_only']\n",
    "    right_only_rows = merged_df[merged_df['_merge'] == 'right_only']\n",
    "    both_rows = merged_df[merged_df['_merge'] == 'both']\n",
    "    right_both_rows = pd.concat([both_rows, right_only_rows])\n",
    "\n",
    "    columns = ['PONDERA', 'PONDII', 'PONDIIO']\n",
    "    df_years = [df_year1, df_year2]\n",
    "\n",
    "    for col in columns:\n",
    "        for df in df_years:\n",
    "            df[f'rel_{col}'] = df[col] / df[col].sum()\n",
    "\n",
    "    df_year1_no_dupl = pd.merge(left_only_rows, df_year1, on=[\"CODUSU\", \"COMPONENTE\"], how='inner')\n",
    "    df_year2_no_dupl = pd.merge(right_both_rows, df_year2, on=[\"CODUSU\", \"COMPONENTE\"], how='inner')\n",
    "\n",
    "    new_columns = ['P21_new', 'P47T_new', 'TOT_P12_new']\n",
    "    df_merged_without_copies = pd.concat([df_year1_no_dupl, df_year2_no_dupl])\n",
    "    df_merged_without_copies[new_columns] = pd.NaT\n",
    "\n",
    "    mask_year1 = df_merged_without_copies['ANO4'] == year1\n",
    "    mask_year2 = df_merged_without_copies['ANO4'] == year2\n",
    "\n",
    "    for col in ['P21', 'P47T', 'TOT_P12']:\n",
    "        df_merged_without_copies.loc[mask_year1 & (df_merged_without_copies[col] != -9), f'{col}_new'] = \\\n",
    "            df_merged_without_copies.loc[mask_year1 & (df_merged_without_copies[col] != -9), col] / conversion_factor\n",
    "        df_merged_without_copies.loc[mask_year1 & (df_merged_without_copies[col] == -9), f'{col}_new'] = \\\n",
    "            df_merged_without_copies.loc[mask_year1 & (df_merged_without_copies[col] == -9), col]\n",
    "\n",
    "    for col in ['P21', 'P47T', 'TOT_P12']:\n",
    "        df_merged_without_copies.loc[mask_year2, f'{col}_new'] = df_merged_without_copies.loc[mask_year2, col]\n",
    "\n",
    "    for col in ['PONDERA', 'PONDII', 'PONDIIO']:\n",
    "        temp_rel = df_merged_without_copies[f'rel_{col}'].div(df_merged_without_copies[f'rel_{col}'].sum())\n",
    "        pob_prom = (df_year1[col].sum() + df_year2[col].sum()) / 2\n",
    "        df_merged_without_copies[f'{col}_new'] = temp_rel * pob_prom\n",
    "\n",
    "    return df_merged_without_copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_calculate_houses(df_year1, df_year2, year1, year2, conversion_factor):\n",
    "\n",
    "    \"\"\"\n",
    "    Combina dos DataFrames de dos anios de la base de hogares de la EPH utilizando las columnas 'CODUSU' y 'NRO_HOGAR'. Realiza cálculos en los datos fusionados.\n",
    "\n",
    "    Args:\n",
    "        df_year1 (pandas.DataFrame): DataFrame para el año 1.\n",
    "        df_year2 (pandas.DataFrame): DataFrame para el año 2.\n",
    "        year1 (int): Año de los datos de df_year1.\n",
    "        year2 (int): Año de los datos de df_year2.\n",
    "        conversion_factor (float): Factor de conversión para la transformación de datos.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame fusionado con los valores calculados. Base pool de base hogares sin replicas.\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    merged_df = pd.merge(df_year1[['CODUSU', 'NRO_HOGAR']], df_year2[['CODUSU', 'NRO_HOGAR']],\n",
    "                         on=['CODUSU', 'NRO_HOGAR'], how='outer', indicator=True)\n",
    "\n",
    "    left_only_rows = merged_df[merged_df['_merge'] == 'left_only']\n",
    "    right_only_rows = merged_df[merged_df['_merge'] == 'right_only']\n",
    "    both_rows = merged_df[merged_df['_merge'] == 'both']\n",
    "    right_both_rows = pd.concat([both_rows, right_only_rows])\n",
    "\n",
    "    columns = ['PONDERA', 'PONDIH']\n",
    "    df_years = [df_year1, df_year2]\n",
    "\n",
    "    for col in columns:\n",
    "        for df in df_years:\n",
    "            df[f'rel_{col}'] = df[col] / df[col].sum()\n",
    "\n",
    "    df_year1_no_dupl = pd.merge(left_only_rows, df_year1, on=[\"CODUSU\", \"NRO_HOGAR\"], how='inner')\n",
    "    df_year2_no_dupl = pd.merge(right_both_rows, df_year2, on=[\"CODUSU\", \"NRO_HOGAR\"], how='inner')\n",
    "\n",
    "    new_columns = ['ITF_new']\n",
    "    df_merged_without_copies = pd.concat([df_year1_no_dupl, df_year2_no_dupl])\n",
    "    df_merged_without_copies[new_columns] = pd.NaT\n",
    "\n",
    "    mask_year1 = df_merged_without_copies['ANO4'] == year1\n",
    "    mask_year2 = df_merged_without_copies['ANO4'] == year2\n",
    "\n",
    "    for col in ['ITF']:\n",
    "        df_merged_without_copies.loc[mask_year1 & (df_merged_without_copies[col] != -9), f'{col}_new'] = \\\n",
    "            df_merged_without_copies.loc[mask_year1 & (df_merged_without_copies[col] != -9), col] / conversion_factor\n",
    "        df_merged_without_copies.loc[mask_year1 & (df_merged_without_copies[col] == -9), f'{col}_new'] = \\\n",
    "            df_merged_without_copies.loc[mask_year1 & (df_merged_without_copies[col] == -9), col]\n",
    "\n",
    "    for col in ['ITF']:\n",
    "        df_merged_without_copies.loc[mask_year2, f'{col}_new'] = df_merged_without_copies.loc[mask_year2, col]\n",
    "\n",
    "    for col in ['PONDERA', 'PONDIH']:\n",
    "        temp_rel = df_merged_without_copies[f'rel_{col}'].div(df_merged_without_copies[f'rel_{col}'].sum())\n",
    "        pob_prom = (df_year1[col].sum() + df_year2[col].sum()) / 2\n",
    "        df_merged_without_copies[f'{col}_new'] = temp_rel * pob_prom\n",
    "\n",
    "    return df_merged_without_copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_factor = 0.546358573\n",
    "df_merged_without_copies_people = merge_and_calculate_people(df_people_2021, df_people_2022, 2021, 2022, conversion_factor)\n",
    "df_merged_without_copies_people.to_csv(pathdata + 'data_output/Base_pool_individuos_solo_con_replicas_actuales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_merged_without_copies_houses = merge_and_calculate_houses(df_houses_2021, df_houses_2022, 2021, 2022, conversion_factor)\n",
    "df_merged_without_copies_houses.to_csv(pathdata + 'data_output/Base_pool_hogares_solo_con_replicas_actuales.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pyenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "509f5cbefa0cafc10dc1461472345186404cff510fb444dcb85013f09977a5b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
